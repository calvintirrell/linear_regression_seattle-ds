{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from random import gauss\n",
    "from lin_reg import best_line\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y std: 0    4.358899\n",
      "dtype: float64 y mean: 0    7.0\n",
      "dtype: float64\n",
      "x std: 0    2.0\n",
      "dtype: float64 x mean: 0    3.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "y = [2, 9, 10]\n",
    "df_y = pd.DataFrame(y)\n",
    "print('y std:',df_y.std(), 'y mean:', df_y.mean())\n",
    "\n",
    "x = [1, 3, 5]\n",
    "df_x = pd.DataFrame(x)\n",
    "print('x std:',df_x.std(), 'x mean:', df_x.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slides [here](https://docs.google.com/presentation/d/1DwCvkgzA0PmdwZJAqD82QJnwcR_AnGcFM74s5QxpDkQ/edit?usp=sharing).\n",
    "\n",
    "The idea of _correlation_ is the simple idea that variables often change _together_. For a simple example, cities with more buses tend to have higher populations.\n",
    "\n",
    "We might observe that, as one variable X increases, so does another Y, OR that as X increases, Y decreases.\n",
    "\n",
    "The _covariance_ describes how two variables co-vary. Note the similarity in the definition to the definition of ordinary variance:\n",
    "\n",
    "## Covariance\n",
    "\n",
    "For two random variables $X$ and $Y$, each with $n$ values:\n",
    "\n",
    "$\\Large\\sigma_{XY} = \\frac{\\Sigma^n_{i = 1}(x_i - \\mu_x)(y_i - \\mu_y)}{n}$ <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.333333333333333"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [1, 3, 5]\n",
    "Y = [2, 9, 10]\n",
    "\n",
    "# Covariance by hand:\n",
    "((1-3) * (2-7) + (3-3) * (9-7) + (5-3) * (10-7)) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.333333333333333"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Better yet: With NumPy:\n",
    "np.cov(X, Y, ddof=0)[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.66666667,  5.33333333],\n",
       "       [ 5.33333333, 12.66666667]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cov(X, Y, ddof=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the value of the covariance is very much a function of the values of X and Y, which can make interpretation difficult. What is wanted is a _standardized_ scale for covariance, hence: _correlation_.\n",
    "\n",
    "## Correlation\n",
    "\n",
    "Pearson Correlation:<br/>$\\Large r_P = \\frac{\\Sigma^n_{i = 1}(x_i - \\mu_x)(y_i - \\mu_y)}{\\sqrt{\\Sigma^n_{i = 1}(x_i - \\mu_x)^2\\Sigma^n_{i = 1}(y_i -\\mu_y)^2}}$\n",
    "\n",
    "Note that we are simply standardizing the covariance by the standard deviations of X and Y (the ($n-1$)'s cancel!).\n",
    "\n",
    "$\\bf{Check}$:\n",
    "\n",
    "<details><summary>\n",
    "What happens if X = Y?\n",
    "</summary>\n",
    "Then numerator = denominator and the correlation = 1!\n",
    "</details>\n",
    "<br/>\n",
    "We'll always have $-1 \\leq r \\leq 1$. (This was the point of standardizing by the standard deviations of X and Y.)\n",
    "\n",
    "A correlation of -1 means that X and Y are perfectly negatively correlated, and a correlation of 1 means that X and Y are perfectly positively correlated.\n",
    "\n",
    "NumPy also has a correlation method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.91766294],\n",
       "       [0.91766294, 1.        ]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(X, Y)[0, 1] == (np.cov(X, Y, ddof=0) / (np.std(X) * np.std(Y)))[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causation\n",
    "\n",
    "_Why_ does it happen that variables correlate? It _may_ be that one is the cause of the other. A city having a high population, for example, probably does have some causal effect on the number of buses that the city has. But this _need not_ be the case, and that is why statisticians are fond of saying that 'correlation is not causation'. An alternative possibility, for example, is that high values of X and Y are _both_ caused by high values of some third factor Z. The size of children's feet, for example, is correlated with their ability to spell, but this is of course NOT because either is a cause of the other. Rather, BOTH are caused by the natural maturing and development of children. As they get older, both their feet and their spelling abilities grow!\n",
    "\n",
    "## Statistical Learning Theory\n",
    "\n",
    "It's important at this point to understand the distinction between dependent and independent variables.\n",
    "\n",
    "Roughly, the independent variable is what can be directly manipulated and the dependent variable is what cannot be (but is nevertheless of great interest). What matters structurally is simply that we understand the dependent variable to be a _function_ of the independent variable(s).\n",
    "\n",
    "This is the proper interpretation of a statistical _model_.\n",
    "\n",
    "Simple idea: We can model correlation with a _line_. As one variable changes, so does the other.\n",
    "\n",
    "This model has two *parameters*: *slope* and *y-intercept*.\n",
    "\n",
    "Unless there's a perfectly (and suspiciously) linear relationship between our predictor(s) and our target, there will  be some sort of **error** or **loss** or **residual**. The best-fit line is constructed by minimizing the sum of the squares of these losses.\n",
    "\n",
    "## Simple Linear Regression\n",
    "\n",
    "The solution for a simple regression best-fit line is as follows:\n",
    "\n",
    "- slope: <br/>$\\Large m = r_P\\frac{\\sigma_y}{\\sigma_x}$\n",
    "\n",
    "- y-intercept:<br/> $\\Large b = \\mu_y - m\\mu_x$\n",
    "\n",
    "Note: The proof of this proceeds by setting the gradient of the loss function equal to 0.\n",
    "\n",
    "Assumptions:\n",
    "\n",
    "1. The relationship between target and predictor(s) is linear. (Of course!)\n",
    "2. The errors are mutually independent. (That is, there is no correlation between any two errors.)\n",
    "3. The errors are normally distributed. (That is, smaller errors are more probable than larger errors, according to the familiar bell curve.)\n",
    "4. The errors are homoskedastic. (That is, the errors have the same variance. The Greek word $\\sigma\\kappa\\epsilon\\delta\\acute{\\alpha}\\nu\\nu\\upsilon\\mu\\iota$ means \"to scatter\".)\n",
    "\n",
    "There is no general requirement that the predictors and the target *themselves* be normally distributed. However: Linear regression can work better if the predictors are normally distributed.\n",
    "\n",
    "[Here](https://www.statisticssolutions.com/assumptions-of-linear-regression/) is a helpful resource on the assumptions of linear regression.\n",
    "\n",
    "Experiment: [Playing with regression line](https://www.desmos.com/calculator/jwquvmikhr) <br/>\n",
    "Limitations: [Anscombe's Quartet](https://www.desmos.com/calculator/paknt6oneh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using best_line\n",
    "\n",
    "best_line(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression\n",
    "\n",
    "The main idea here is pretty simple. Whereas, in simple linear regression we took our dependent variable to be a function only of a single independent variable, here we'll be taking the dependent variable to be a function of multiple independent variables.\n",
    "\n",
    "Our regression equation, then, instead of looking like $\\hat{y} = mx + b$, will now look like:\n",
    "\n",
    "$\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + ... + \\hat{\\beta}_nx_n$.\n",
    "\n",
    "Remember that the hats ( $\\hat{}$ ) indicate parameters that are estimated.\n",
    "\n",
    "Is this still a best-fit *line*? Well, no. What does the graph of, say, z = x + y look like? [Here's](https://academo.org/demos/3d-surface-plotter/) a 3d-plotter. (Of course, once we get x's with subscripts beyond 2 it's going to be very hard to visualize. But in practice linear regressions can make use of dozens or even of hundreds of independent variables!)\n",
    "\n",
    "I want to focus here more on what coding a multiple regression looks like in Python. But you might be wondering: Is it possible to calculate the betas by hand?\n",
    "\n",
    "Yes! See [here](https://stattrek.com/multiple-regression/regression-coefficients.aspx) for a nice explanation and example.\n",
    "\n",
    "We'll focus more directly on matrix mathematics later in the course.\n",
    "\n",
    "## Dealing with Categorical Variables\n",
    "\n",
    "One issue we'd like to resolve is what to do with categorical variables, i.e. variables that represent categories rather than continua. In a Pandas DataFrame, these columns may well have strings or objects for values, but they need not. A certain heart-disease dataset from Kaggle, for example, has a target variable that takes values 0-4, each representing a different stage of heart disease.\n",
    "\n",
    "### Dummying\n",
    "\n",
    "One very effective way of dealing with categorical variables is to dummy them out. What this involves is making a new column for _each categorical value in the column we're dummying out_.\n",
    "\n",
    "These new columns will be filled only with 0's and 1's, a 1 representing the presence of the relevant categorical value.\n",
    "\n",
    "Let's look at a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "comma_use = pd.read_csv('data/comma-survey.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more on this dataset see [here](https://fivethirtyeight.com/features/elitist-superfluous-or-popular-we-polled-americans-on-the-oxford-comma/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RespondentID</th>\n",
       "      <th>In your opinion, which sentence is more gramatically correct?</th>\n",
       "      <th>Prior to reading about it above, had you heard of the serial (or Oxford) comma?</th>\n",
       "      <th>How much, if at all, do you care about the use (or lack thereof) of the serial (or Oxford) comma in grammar?</th>\n",
       "      <th>How would you write the following sentence?</th>\n",
       "      <th>When faced with using the word \"data\", have you ever spent time considering if the word was a singular or plural noun?</th>\n",
       "      <th>How much, if at all, do you care about the debate over the use of the word \"data\" as a singluar or plural noun?</th>\n",
       "      <th>In your opinion, how important or unimportant is proper use of grammar?</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Household Income</th>\n",
       "      <th>Education</th>\n",
       "      <th>Location (Census Region)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3292953864</td>\n",
       "      <td>It's important for a person to be honest, kind...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Some</td>\n",
       "      <td>Some experts say it's important to drink milk,...</td>\n",
       "      <td>No</td>\n",
       "      <td>Not much</td>\n",
       "      <td>Somewhat important</td>\n",
       "      <td>Male</td>\n",
       "      <td>30-44</td>\n",
       "      <td>$50,000 - $99,999</td>\n",
       "      <td>Bachelor degree</td>\n",
       "      <td>South Atlantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3292950324</td>\n",
       "      <td>It's important for a person to be honest, kind...</td>\n",
       "      <td>No</td>\n",
       "      <td>Not much</td>\n",
       "      <td>Some experts say it's important to drink milk,...</td>\n",
       "      <td>No</td>\n",
       "      <td>Not much</td>\n",
       "      <td>Somewhat unimportant</td>\n",
       "      <td>Male</td>\n",
       "      <td>30-44</td>\n",
       "      <td>$50,000 - $99,999</td>\n",
       "      <td>Graduate degree</td>\n",
       "      <td>Mountain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3292942669</td>\n",
       "      <td>It's important for a person to be honest, kind...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Some</td>\n",
       "      <td>Some experts say it's important to drink milk,...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Not at all</td>\n",
       "      <td>Very important</td>\n",
       "      <td>Male</td>\n",
       "      <td>30-44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>East North Central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3292932796</td>\n",
       "      <td>It's important for a person to be honest, kind...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Some</td>\n",
       "      <td>Some experts say it's important to drink milk,...</td>\n",
       "      <td>No</td>\n",
       "      <td>Some</td>\n",
       "      <td>Somewhat important</td>\n",
       "      <td>Male</td>\n",
       "      <td>18-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Less than high school degree</td>\n",
       "      <td>Middle Atlantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3292932522</td>\n",
       "      <td>It's important for a person to be honest, kind...</td>\n",
       "      <td>No</td>\n",
       "      <td>Not much</td>\n",
       "      <td>Some experts say it's important to drink milk,...</td>\n",
       "      <td>No</td>\n",
       "      <td>Not much</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RespondentID In your opinion, which sentence is more gramatically correct?  \\\n",
       "0    3292953864  It's important for a person to be honest, kind...              \n",
       "1    3292950324  It's important for a person to be honest, kind...              \n",
       "2    3292942669  It's important for a person to be honest, kind...              \n",
       "3    3292932796  It's important for a person to be honest, kind...              \n",
       "4    3292932522  It's important for a person to be honest, kind...              \n",
       "\n",
       "  Prior to reading about it above, had you heard of the serial (or Oxford) comma?  \\\n",
       "0                                                Yes                                \n",
       "1                                                 No                                \n",
       "2                                                Yes                                \n",
       "3                                                Yes                                \n",
       "4                                                 No                                \n",
       "\n",
       "  How much, if at all, do you care about the use (or lack thereof) of the serial (or Oxford) comma in grammar?  \\\n",
       "0                                               Some                                                             \n",
       "1                                           Not much                                                             \n",
       "2                                               Some                                                             \n",
       "3                                               Some                                                             \n",
       "4                                           Not much                                                             \n",
       "\n",
       "         How would you write the following sentence?  \\\n",
       "0  Some experts say it's important to drink milk,...   \n",
       "1  Some experts say it's important to drink milk,...   \n",
       "2  Some experts say it's important to drink milk,...   \n",
       "3  Some experts say it's important to drink milk,...   \n",
       "4  Some experts say it's important to drink milk,...   \n",
       "\n",
       "  When faced with using the word \"data\", have you ever spent time considering if the word was a singular or plural noun?  \\\n",
       "0                                                 No                                                                       \n",
       "1                                                 No                                                                       \n",
       "2                                                Yes                                                                       \n",
       "3                                                 No                                                                       \n",
       "4                                                 No                                                                       \n",
       "\n",
       "  How much, if at all, do you care about the debate over the use of the word \"data\" as a singluar or plural noun?  \\\n",
       "0                                           Not much                                                                \n",
       "1                                           Not much                                                                \n",
       "2                                         Not at all                                                                \n",
       "3                                               Some                                                                \n",
       "4                                           Not much                                                                \n",
       "\n",
       "  In your opinion, how important or unimportant is proper use of grammar?  \\\n",
       "0                                 Somewhat important                        \n",
       "1                               Somewhat unimportant                        \n",
       "2                                     Very important                        \n",
       "3                                 Somewhat important                        \n",
       "4                                                NaN                        \n",
       "\n",
       "  Gender    Age   Household Income                     Education  \\\n",
       "0   Male  30-44  $50,000 - $99,999               Bachelor degree   \n",
       "1   Male  30-44  $50,000 - $99,999               Graduate degree   \n",
       "2   Male  30-44                NaN                           NaN   \n",
       "3   Male  18-29                NaN  Less than high school degree   \n",
       "4    NaN    NaN                NaN                           NaN   \n",
       "\n",
       "  Location (Census Region)  \n",
       "0           South Atlantic  \n",
       "1                 Mountain  \n",
       "2       East North Central  \n",
       "3          Middle Atlantic  \n",
       "4                      NaN  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comma_use.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1129, 13)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comma_use.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "comma_use.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(825, 13)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comma_use.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try using sklearn's OneHotEncoder to create our dummy columns:\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(drop='first')\n",
    "comma_trans = ohe.fit_transform(comma_use.drop('RespondentID', axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could we have used ```pd.get_dummies()``` instead?\n",
    "\n",
    "Well, yes. And in fact ```get_dummies()``` is in some ways easier; for one thing, it's built right into Pandas. But there are drawbacks with it as well. See the *bottom* of [this link](https://stackoverflow.com/questions/36631163/pandas-get-dummies-vs-sklearns-onehotencoder-what-are-the-pros-and-cons) for a good explanation.\n",
    "\n",
    "So what did the encoder do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 1., 0., ..., 1., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comma_trans.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"x0_It's important for a person to be honest, kind, and loyal.\",\n",
       "       'x1_Yes', 'x2_Not at all', 'x2_Not much', 'x2_Some',\n",
       "       \"x3_Some experts say it's important to drink milk, but the data is inconclusive.\",\n",
       "       'x4_Yes', 'x5_Not at all', 'x5_Not much', 'x5_Some',\n",
       "       'x6_Somewhat important', 'x6_Somewhat unimportant',\n",
       "       'x6_Very important', 'x6_Very unimportant', 'x7_Male', 'x8_30-44',\n",
       "       'x8_45-60', 'x8_> 60', 'x9_$100,000 - $149,999', 'x9_$150,000+',\n",
       "       'x9_$25,000 - $49,999', 'x9_$50,000 - $99,999',\n",
       "       'x10_Graduate degree', 'x10_High school degree',\n",
       "       'x10_Less than high school degree',\n",
       "       'x10_Some college or Associate degree', 'x11_East South Central',\n",
       "       'x11_Middle Atlantic', 'x11_Mountain', 'x11_New England',\n",
       "       'x11_Pacific', 'x11_South Atlantic', 'x11_West North Central',\n",
       "       'x11_West South Central'], dtype=object)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0_It's important for a person to be honest, kind, and loyal.</th>\n",
       "      <th>x1_Yes</th>\n",
       "      <th>x2_Not at all</th>\n",
       "      <th>x2_Not much</th>\n",
       "      <th>x2_Some</th>\n",
       "      <th>x3_Some experts say it's important to drink milk, but the data is inconclusive.</th>\n",
       "      <th>x4_Yes</th>\n",
       "      <th>x5_Not at all</th>\n",
       "      <th>x5_Not much</th>\n",
       "      <th>x5_Some</th>\n",
       "      <th>...</th>\n",
       "      <th>x10_Less than high school degree</th>\n",
       "      <th>x10_Some college or Associate degree</th>\n",
       "      <th>x11_East South Central</th>\n",
       "      <th>x11_Middle Atlantic</th>\n",
       "      <th>x11_Mountain</th>\n",
       "      <th>x11_New England</th>\n",
       "      <th>x11_Pacific</th>\n",
       "      <th>x11_South Atlantic</th>\n",
       "      <th>x11_West North Central</th>\n",
       "      <th>x11_West South Central</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   x0_It's important for a person to be honest, kind, and loyal.  x1_Yes  \\\n",
       "0                                                0.0                 1.0   \n",
       "1                                                1.0                 0.0   \n",
       "2                                                1.0                 0.0   \n",
       "3                                                1.0                 1.0   \n",
       "4                                                1.0                 1.0   \n",
       "\n",
       "   x2_Not at all  x2_Not much  x2_Some  \\\n",
       "0            0.0          0.0      1.0   \n",
       "1            0.0          1.0      0.0   \n",
       "2            0.0          0.0      0.0   \n",
       "3            0.0          0.0      0.0   \n",
       "4            0.0          0.0      0.0   \n",
       "\n",
       "   x3_Some experts say it's important to drink milk, but the data is inconclusive.  \\\n",
       "0                                                1.0                                 \n",
       "1                                                1.0                                 \n",
       "2                                                0.0                                 \n",
       "3                                                1.0                                 \n",
       "4                                                0.0                                 \n",
       "\n",
       "   x4_Yes  x5_Not at all  x5_Not much  x5_Some  ...  \\\n",
       "0     0.0            0.0          1.0      0.0  ...   \n",
       "1     0.0            0.0          1.0      0.0  ...   \n",
       "2     1.0            0.0          0.0      1.0  ...   \n",
       "3     1.0            0.0          0.0      1.0  ...   \n",
       "4     0.0            0.0          0.0      0.0  ...   \n",
       "\n",
       "   x10_Less than high school degree  x10_Some college or Associate degree  \\\n",
       "0                               0.0                                   0.0   \n",
       "1                               0.0                                   0.0   \n",
       "2                               0.0                                   1.0   \n",
       "3                               0.0                                   1.0   \n",
       "4                               0.0                                   1.0   \n",
       "\n",
       "   x11_East South Central  x11_Middle Atlantic  x11_Mountain  x11_New England  \\\n",
       "0                     0.0                  0.0           0.0              0.0   \n",
       "1                     0.0                  0.0           1.0              0.0   \n",
       "2                     0.0                  0.0           0.0              1.0   \n",
       "3                     0.0                  0.0           0.0              0.0   \n",
       "4                     0.0                  0.0           0.0              0.0   \n",
       "\n",
       "   x11_Pacific  x11_South Atlantic  x11_West North Central  \\\n",
       "0          0.0                 1.0                     0.0   \n",
       "1          0.0                 0.0                     0.0   \n",
       "2          0.0                 0.0                     0.0   \n",
       "3          1.0                 0.0                     0.0   \n",
       "4          0.0                 0.0                     0.0   \n",
       "\n",
       "   x11_West South Central  \n",
       "0                     0.0  \n",
       "1                     0.0  \n",
       "2                     0.0  \n",
       "3                     0.0  \n",
       "4                     0.0  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(comma_trans.todense(), columns=ohe.get_feature_names())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wine Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = pd.read_csv('data/wine.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "Let's imagine that I'm going to try to predict wine quality based on the other features.\n",
    "\n",
    "Now: Which columns (predictors) should I choose? There are 12 predictors I could choose from For each of these predictors, I could either use it or not use it in my model, which means that there are 2^12 = 4096 different models I could construct! Well, okay, one of these is the \"empty model\" with no predictors in it. But there are still 4095 models from which I can choose.\n",
    "\n",
    "How can I decide which predictors to use in my model?\n",
    "\n",
    "We'll explore a few methods here. For more on feature selection, see [this post](https://towardsdatascience.com/the-5-feature-selection-algorithms-every-data-scientist-need-to-know-3a6b566efd2).\n",
    "\n",
    "### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use the .corr() DataFrame method to find out about the\n",
    "# correlation values between all pairs of variables!\n",
    "\n",
    "wine.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(8, 8)})\n",
    "\n",
    "# Use the .heatmap method to depict the relationships visually!\n",
    "sns.heatmap(wine.corr());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the correlations with 'quality'\n",
    "# (our dependent variable) in particular.\n",
    "\n",
    "wine.corr()['quality'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's choose 'alcohol' and 'density'.\n",
    "\n",
    "wine_preds = wine[['alcohol', 'density']]\n",
    "wine_target = wine['quality']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Regression in StatsModels\n",
    "\n",
    "Statsmodels offers a highly descriptive report of the fit of a regression model. Let's generate a simple regression and then analyze the report!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's try data that fit a straight line perfectly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(20)\n",
    "y = 3*x + 1         # Note that we can do this only because x is a NumPy array!\n",
    "\n",
    "sm.OLS(y, sm.add_constant(x)).fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\rightarrow$Now let's add a little noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(20)\n",
    "y = np.array([3*pt + 1 + gauss(mu=0, sigma=5) for pt in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(columns=['x', 'y'])\n",
    "\n",
    "df2['x'] = x\n",
    "df2['y'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.formula.ols(formula='y~x', data=df2).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note the difference between `sm.OLS()` and `sm.formula.ols()`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.graphics.plot_regress_exog(model, 'x', fig=plt.figure(figsize=(12, 8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we interpret this report?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coefficient of Determination\n",
    "\n",
    "Very often a data scientist will calculate $R^2$, the *coefficient of determination*, as a measure of how well the model fits the data.\n",
    "\n",
    "$R^2$ for a model is ultimately a _relational_ notion. It's a measure of goodness of fit _relative_ to a (bad) baseline model. This bad baseline model is simply the horizontal line $y = \\mu_Y$, for dependent variable $Y$.\n",
    "\n",
    "The actual calculation of $R^2$ is: <br/> $\\Large R^2\\equiv 1-\\frac{\\Sigma_i(y_i - \\hat{y}_i)^2}{\\Sigma_i(y_i - \\bar{y})^2}$.\n",
    "\n",
    "$R^2$ is a measure of how much variation in the dependent variable your model explains.\n",
    "\n",
    "### Adjusted $R^2$\n",
    "\n",
    "There are some theoretical [objections](https://data.library.virginia.edu/is-r-squared-useless/) to using $R^2$ as an evaluator of a regression model.\n",
    "\n",
    "One objection is that, if we add another predictor to our model, $R^2$ can only *increase*! (It could hardly be that with more features I'd be able to account for *less* of the variation in the dependent variable than I could with the smaller set of features.)\n",
    "\n",
    "One improvement is **adjusted $R^2$**: <br/> $\\Large R^2_{adj.}\\equiv 1 - \\frac{(1 - R^2)(n - 1)}{n - m - 1}$, where:\n",
    "\n",
    "- n is the number of data points; and\n",
    "- m is the number of predictors.\n",
    "\n",
    "This can be a better indicator of the quality of a regression model. For more, see [here](https://www.statisticshowto.datasciencecentral.com/adjusted-r2/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that $R^2$ *can* be negative!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "X, y = make_regression()\n",
    "\n",
    "bad_pred = np.mean(y) * np.ones(len(y))\n",
    "worse_pred = (np.mean(y) - 100) * np.ones(len(y))\n",
    "\n",
    "print(r2_score(y, bad_pred))\n",
    "print(r2_score(y, worse_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Regression Statistics\n",
    "\n",
    "What else do we have in this report?\n",
    "\n",
    "- **F-statistic**: The F-test measures the significance of your model relative to a model in which all coefficients are 0, i.e. relative to a model that says there is no correlation whatever between the predictors and the target. <br/><br/>\n",
    "- **Log-Likelihood**: The probability in question is the probability of seeing these data points, *given* the model parameter values. The higher this is, the more our data conform to our model and so the better our fit. AIC and BIC are related to the log-likelihood; we'll talk about those later. <br/><br/>\n",
    "- **coef**: These are the betas as calculated by the least-squares regression. We also have p-values and 95%-confidence intervals. <br/><br/>\n",
    "- **Omnibus**: This is a test for error normality. The probability is the chance that the errors are normally distributed. <br/><br/>\n",
    "- **Durbin-Watson**: This is a test for error homoskedasticity. We're looking for values between ~1.5 and ~2.5. <br/><br/>\n",
    "- **Jarque-Bera**: This is another test for error normality. <br/><br/>\n",
    "- **Cond. No.**: The condition number tests for independence of the predictors. Lower scores are better. When the predictors are *not* independent, we can run into problems of multicollinearity. For more on the condition number, see [here](https://stats.stackexchange.com/questions/168259/how-do-you-interpret-the-condition-number-of-a-correlation-matrix).\n",
    "\n",
    "For more on statsmodels regression statistics, see [here](https://www.accelebrate.com/blog/interpreting-results-from-linear-regression-is-the-data-appropriate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multicollinearity\n",
    "\n",
    "Multicollinearity describes the correlation between distinct predictors. Why might high multicollinearity be a problem for interpreting a linear regression model?\n",
    "\n",
    "It's problematic for statistics in an inferential mode because, if $x_1$ and $x_2$ are highly correlated with $y$ but also *with each other*, then it will be very difficult to tease apart the effects of $x_1$ on $y$ and the effects of $x_2$ on $y$. If I really want to have a good sense of the effect of $x_1$ on $y$, then I'd like to vary $x_1$ while keeping the other features constant. But if $x_1$ is highly correlated with $x_2$ then this will be a practically impossible exercise!\n",
    "\n",
    "We will return to this topic again. For more, see [this post](https://towardsdatascience.com/https-towardsdatascience-com-multicollinearity-how-does-it-create-a-problem-72956a49058)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "\n",
    "Before we construct a linear regression, let's *scale* our columns by z-scores. Why?\n",
    "\n",
    "In a word, it's useful to have all of our variables be on the same scale, so that the resulting coefficients are easier to interpret. If, moreover, the scales of the variables are very different one from another, then some of the coefficients may end up on very large or very tiny scales.\n",
    "\n",
    "For more on this, see [this post](https://stats.stackexchange.com/questions/32649/some-of-my-predictors-are-on-very-different-scales-do-i-need-to-transform-them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_preds_scaled = (wine_preds - np.mean(wine_preds)) / np.std(wine_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a model with our wine dataset now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictors = sm.add_constant(wine_preds_scaled)\n",
    "model = sm.OLS(wine_target, predictors).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_preds2 = wine[['alcohol', 'density', 'volatile acidity']]\n",
    "\n",
    "wine_preds2_scaled = (wine_preds2 - np.mean(wine_preds2)) / np.std(wine_preds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = sm.add_constant(wine_preds2_scaled)\n",
    "model = sm.OLS(np.asarray(wine_target), predictors).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Regression in Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a StandardScaler object to scale our data for us.\n",
    "ss = StandardScaler()\n",
    "\n",
    "\n",
    "# Now we'll apply it to our data by using the .fit() and .transform() methods.\n",
    "\n",
    "ss.fit(wine_preds2)\n",
    "\n",
    "wine_preds2_scaled = ss.transform(wine_preds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can fit a LinearRegression object to our training data!\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(wine_preds2_scaled, wine_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use the .coef_ attribute to recover the results\n",
    "# of the regression.\n",
    "\n",
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination\n",
    "\n",
    "The idea behind recursive feature elimination is to start with all predictive features and then build down to a small set of features slowly, by eliminating the features with the lowest coefficients.\n",
    "\n",
    "That is:\n",
    "1. Start with a model with _all_ $n$ predictors;\n",
    "2. find the predictor with the smallest coefficient;\n",
    "3. throw that predictor out and build a model with the remining $n-1$ predictors;\n",
    "4. set $n = n-1$ and repeat until $n-1$ has the value you want!\n",
    "\n",
    "### Recursive Feature Elimination in Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "lr_rfe = LinearRegression()\n",
    "select = RFE(lr_rfe, n_features_to_select=1)\n",
    "select = select.fit(X = wine.drop('quality', axis=1), y = wine['quality'])\n",
    "\n",
    "select.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "select.ranking_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caution: RFE is probably not a good strategy if your initial dataset has many predictors. It will likely be easier to start with a *simple* model and then slowly increase its complexity. This is also good advice for when you're first getting your feet wet with `sklearn`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we can get better performance if we multiply features together. Consider the following dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.read_csv('data/Advertising.csv', index_col=0)\n",
    "\n",
    "sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd like to try to understand sales as a function of spending on various media (TV, radio, newspaper). We could check correlations of sales with the different features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.corr()['sales']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation with TV spending is pretty high. But look what happens when we add a new column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales['TV+Radio'] = sales['TV'] * sales['radio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.corr()['sales']['TV+Radio']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation here is amazing! Let's make ourselves a plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(sales['TV+Radio'], sales['sales']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, it's not easy to tell when such products of features will be so fruitful. Moreover, there is room for concern about violating regression's demand for feature independence. At the very least, we would probably not want to include a product *and the individual features themselves* in a final model, not if our goal is to understand what's really responsible for fluctuations in our target variable.\n",
    "\n",
    "## Sklearn Metrics\n",
    "\n",
    "The metrics module in sklearn has a number of metrics that we can use to meaure the accuracy of our model, including the $R^2$ score, the mean absolute error and the mean squared error. Note that the default 'score' on our model object is the $R^2$ score. Let's go back to our wine dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.r2_score(wine_target, lr.predict(wine_preds2_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure this metric is properly calibrated. If we put simply $\\bar{y}$ as our prediction, then we should get an $R^2$ score of *0*. And if we predict, say, $\\bar{y} + 1$, then we should get a *negative* $R^2$ score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_quality = np.mean(wine_target)\n",
    "num = len(wine_target)\n",
    "\n",
    "metrics.r2_score(wine_target, avg_quality * np.ones(num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.r2_score(wine_target, (avg_quality + 1) * np.ones(num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.mean_absolute_error(Y_test, lr.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.mean_squared_error(Y_test, lr.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting X so that the intercept term of the best-fit line will be 0\n",
    "X = np.array([1.5, 3.5, 5.5])\n",
    "Y = np.array([2, 9, 10])\n",
    "\n",
    "model = LinearRegression().fit(X.reshape(-1, 1), Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sse(m):\n",
    "    line = m*X\n",
    "    err = sum(x**2 for x in [line - model.predict(X.reshape(-1, 1))])\n",
    "    return sum(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ms = np.linspace(0, 5, 100)\n",
    "ys = [sse(m) for m in ms]\n",
    "\n",
    "ax.plot(ms, ys);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going 3d to plot error as a function of both m and b\n",
    "from mpl_toolkits import mplot3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_sse(m, x, b, y):\n",
    "    \"\"\"\n",
    "    This function returns the sum of squared errors for\n",
    "    a target y and a linear estimate mx + b.\n",
    "    \"\"\"\n",
    "    return len(x) * metrics.mean_squared_error(y, m*x + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going back to our original example\n",
    "X_sample = np.array([1, 3, 5])\n",
    "Y_sample = np.array([2, 9, 10])\n",
    "\n",
    "# This should be our minimum error\n",
    "new_sse(2, X_sample, 1, Y_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = np.linspace(-3, 7, 100)\n",
    "bs = np.linspace(-5, 5, 100)\n",
    "\n",
    "X_grid, Y_grid = np.meshgrid(ms, bs)\n",
    "\n",
    "Z = np.array([[new_sse(m, X_sample, b, Y_sample) for m in ms] for b in bs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_errs = {}\n",
    "for m in ms:\n",
    "    m_errs[m] = new_sse(m, X_sample, 1, Y_sample)\n",
    "print(min(m_errs.values()))\n",
    "for k in m_errs:\n",
    "    if m_errs[k] == min(m_errs.values()):\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_errs = {}\n",
    "for b in bs:\n",
    "    b_errs[b] = new_sse(2, X_sample, b, Y_sample)\n",
    "print(min(b_errs.values()))\n",
    "for k in b_errs:\n",
    "    if b_errs[k] == min(b_errs.values()):\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(X_grid, Y_grid, Z)\n",
    "ax.set_xlabel('slope')\n",
    "ax.set_ylabel('y-intercept')\n",
    "ax.set_zlabel('sum of squared errors')\n",
    "plt.title('Error as a function of slope and y-intercept');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.contour3D(X_grid, Y_grid, Z, 200)\n",
    "ax.set_xlabel('slope')\n",
    "ax.set_ylabel('y-intercept')\n",
    "ax.set_zlabel('sum of squared errors')\n",
    "plt.title('Error as a function of slope and y-intercept');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression on Comma Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comma_use.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll try to predict the first column of df: the extent to which the person accepts the sentence\n",
    "# without the Oxford comma as more grammatically correct.\n",
    "\n",
    "comma_target = df['x0_It\\'s important for a person to be honest, kind and loyal.']\n",
    "\n",
    "comma_predictors = df[['x8_18-29', 'x8_30-44',\n",
    "       'x8_45-60', 'x8_> 60', 'x9_$0 - $24,999', 'x9_$100,000 - $149,999',\n",
    "       'x9_$150,000+', 'x9_$25,000 - $49,999', 'x9_$50,000 - $99,999']]\n",
    "\n",
    "comma_lr = LinearRegression()\n",
    "\n",
    "comma_lr.fit(comma_predictors, comma_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comma_lr.score(comma_predictors, comma_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comma_lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()['x0_It\\'s important for a person to be honest, kind and loyal.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
